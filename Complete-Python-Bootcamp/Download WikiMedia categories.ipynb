{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Downloads'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the full url of wikipedia category: http://cdn.spsdarj.org/index.php/Main_Page\n",
      "Enter The folder name inside which to download: cdn\n",
      "Downloading page http://cdn.spsdarj.org/index.php/Main_Page...\n",
      "Could not find any images.\n",
      "Done. \n",
      "\n",
      "Pics were downloaded to: D:\\Downloads\\cdn\n"
     ]
    }
   ],
   "source": [
    "#! python3\n",
    "# downloadwikimedia.py - Downloads every single image from a category\n",
    "\n",
    "'''\n",
    "There are 2 steps this program takes. \n",
    "When you enter the category url, it extracts link for all pics' individual pages.\n",
    "Then from each page it extracts link for full resolution image.\n",
    "Then it downloads the image.\n",
    "\n",
    "Along the way, there are 2 try catch blocks.\n",
    "try:\n",
    "    download the individual pic's page\n",
    "    try:\n",
    "        download the image\n",
    "    except:\n",
    "        the image couldn't be downloaded\n",
    "except:\n",
    "    pic page not found\n",
    "'''\n",
    "\n",
    "import requests, os, bs4\n",
    "\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',\n",
    "    }\n",
    "\n",
    "url = dirname = ''\n",
    "errorCount = 0\n",
    "\n",
    "while url == '' or not url.startswith('https://commons.wikimedia.org/wiki/Category:'):\n",
    "    # https://commons.wikimedia.org/wiki/Category:St._Paul%27s_School,_Darjeeling\n",
    "    url = input('Enter the full url of wikipedia category: ') # starting url\n",
    "while dirname =='':\n",
    "    dirname = input('Enter The folder name inside which to download: ')\n",
    "os.makedirs(dirname, exist_ok=True)   # store comics in ./dirname\n",
    "\n",
    "# Download the page.\n",
    "print('Downloading page %s...' % url)\n",
    "res = requests.get(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# TODO: Find the URL of the comic image.\n",
    "picElem = soup.select('li > div > div.gallerytext > a')\n",
    "if picElem == []:\n",
    "     print('Could not find any images.')\n",
    "else:\n",
    "    for url in picElem:\n",
    "        picTitle = url.contents[0]\n",
    "        picURL= 'https://commons.wikimedia.org/' + url.get('href')\n",
    "        \n",
    "        try: \n",
    "            res2 = requests.get(picURL, headers=headers)\n",
    "            res2.raise_for_status()\n",
    "            soup2 = bs4.BeautifulSoup(res2.text, 'html.parser')\n",
    "\n",
    "            originalFile=soup2.select('#mw-content-text > div.fullMedia > p > a')[0].get('href')\n",
    "            if originalFile == []:\n",
    "                print('Could not find any images.')\n",
    "            else:\n",
    "                try:\n",
    "                    # Download the image.\n",
    "                    print('Downloading image %s...' % (picTitle))\n",
    "                    res = requests.get(originalFile)\n",
    "                    res.raise_for_status()\n",
    "                except requests.exceptions.MissingSchema:\n",
    "                    # start the next iteration of the loop\n",
    "                    errorCount+=1\n",
    "                    continue\n",
    "                \n",
    "                #Save the image to ./spsdarj\n",
    "                imageFile = open(os.path.join(dirname, picTitle), 'wb')\n",
    "                for chunk in res.iter_content(100000):\n",
    "                    imageFile.write(chunk)\n",
    "                imageFile.close()\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            errorCount+=1\n",
    "\n",
    "print('Done. \\n\\nPics were downloaded to: '+ os.path.join(os.getcwd(), dirname))\n",
    "if errorCount>0:\n",
    "    print('There were however, '+str(errorCount)+' error(s) while downloading.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
