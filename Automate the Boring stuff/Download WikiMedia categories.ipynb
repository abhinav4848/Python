{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('D:/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the full url of wikipedia category: https://commons.wikimedia.org/wiki/Karnataka_Waterfalls\n",
      "Enter The folder name inside which to download: india\n",
      "Downloading page https://commons.wikimedia.org/wiki/Karnataka_Waterfalls ...\n",
      "Downloading image Gokak 1.jpg...\n",
      "Downloading image Gokak 3.jpg...\n",
      "Done. \n",
      "\n",
      "Pics were downloaded to: D:\\Downloads\\india\n"
     ]
    }
   ],
   "source": [
    "#! python3\n",
    "# downloadwikimedia.py - Downloads every single image from a category\n",
    "\n",
    "'''\n",
    "There are 2 steps this program takes. \n",
    "When you enter the category url, it extracts link for all pics' individual pages.\n",
    "Then from each page it extracts link for full resolution image.\n",
    "Then it downloads the image.\n",
    "\n",
    "Along the way, there are 2 try catch blocks.\n",
    "try:\n",
    "    download the individual pic's page\n",
    "    try:\n",
    "        download the image\n",
    "    except:\n",
    "        the image couldn't be downloaded\n",
    "except:\n",
    "    pic page not found\n",
    "'''\n",
    "\n",
    "import requests, os, bs4\n",
    "\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',\n",
    "    }\n",
    "\n",
    "url = dirname = ''\n",
    "errorCount = 0\n",
    "\n",
    "while url == '' or not url.startswith('https://commons.wikimedia.org/wiki/'):\n",
    "    # https://commons.wikimedia.org/wiki/Category:St._Paul%27s_School,_Darjeeling\n",
    "    # https://commons.wikimedia.org/wiki/Commons:Picture_of_the_Year\n",
    "    url = input('Enter the full url of wikipedia category: ') # starting url\n",
    "\n",
    "while dirname =='':\n",
    "    dirname = input('Enter The folder name inside which to download: ')\n",
    "os.makedirs(dirname, exist_ok=True)   # store comics in ./dirname\n",
    "\n",
    "# Download the page.\n",
    "print('Downloading page %s ...' % url)\n",
    "res = requests.get(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "if url.startswith('https://commons.wikimedia.org/wiki/Category:'):\n",
    "    picElem = soup.select('li > div > div.gallerytext > a')\n",
    "else:\n",
    "    # assume url.startswith('https://commons.wikimedia.org/wiki/'):\n",
    "    picElem = soup.select('div.thumb > div > a')\n",
    "\n",
    "if picElem == []:\n",
    "     print('Could not find any images.')\n",
    "else:\n",
    "    for url in picElem:\n",
    "        picURL= 'https://commons.wikimedia.org/' + url.get('href')\n",
    "        \n",
    "        try: \n",
    "            res2 = requests.get(picURL, headers=headers)\n",
    "            res2.raise_for_status()\n",
    "            soup2 = bs4.BeautifulSoup(res2.text, 'html.parser')\n",
    "            \n",
    "            # get id 'first heading', it returns a list, take 0th index, extract contents, remove 'File:' string\n",
    "            picTitle = soup2.select('#firstHeading')[0].contents[0][5:]\n",
    "            \n",
    "            # get url of original file\n",
    "            originalFile=soup2.select('#mw-content-text > div.fullMedia > p > a')[0].get('href')\n",
    "            \n",
    "            if originalFile == []:\n",
    "                print('Could not find any images.')\n",
    "            else:\n",
    "                if os.path.isfile(os.path.join(dirname,picTitle)):\n",
    "                    # file already exists\n",
    "                    print(f'{picTitle} already exists in {dirname}')\n",
    "                    continue\n",
    "                else:\n",
    "                    #download the file\n",
    "                    try:\n",
    "                        # Download the image.\n",
    "                        print('Downloading image %s...' % (picTitle))\n",
    "                        res = requests.get(originalFile)\n",
    "                        res.raise_for_status()\n",
    "                    except requests.exceptions.MissingSchema:\n",
    "                        # start the next iteration of the loop\n",
    "                        errorCount+=1\n",
    "                        continue\n",
    "\n",
    "                    #Save the image to ./dirname\n",
    "                    imageFile = open(os.path.join(dirname, picTitle), 'wb')\n",
    "                    for chunk in res.iter_content(100000):\n",
    "                        imageFile.write(chunk)\n",
    "                    imageFile.close()\n",
    "                    \n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "            errorCount+=1\n",
    "\n",
    "print('Done. \\n\\nPics were downloaded to: '+ os.path.join(os.getcwd(), dirname))\n",
    "if errorCount>0:\n",
    "    print('There were however, '+str(errorCount)+' error(s) while downloading.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
